{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch \n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0') \n",
    "env = JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_states = env.observation_space\n",
    "N_actions = env.action_space\n",
    "print('The state space has the following shape :', N_states)\n",
    "print('The action space has the following shape :', N_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention = 5\n",
    "buffer_capacity = 10000\n",
    "batch_size = 16\n",
    "to_skip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processor(raw_image):                                   \n",
    "    raw_image_trans = np.transpose(raw_image,(2,0,1))               \n",
    "    raw_image_trans_copy = np.copy(raw_image_trans)                 \n",
    "    raw_image_tensor = torch.from_numpy(raw_image_trans_copy)       \n",
    "    grayscaler_func = T.Grayscale(num_output_channels=1)\n",
    "    grayscaled_image_tensor = grayscaler_func(raw_image_tensor)     \n",
    "    resizer_func = T.Resize((84,84))                                  \n",
    "    resized_image_tensor = resizer_func(grayscaled_image_tensor)    \n",
    "    processed_image_tensor = np.transpose(resized_image_tensor,(1,2,0))\n",
    "    return(processed_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_retainer(current_retainer,new_image):\n",
    "    processed_new_image = image_processor(new_image)\n",
    "    dim_0 = np.shape(current_retainer)[0]\n",
    "    dim_1 = np.shape(current_retainer)[1]\n",
    "    for i in np.arange(retention-1,0,-1):\n",
    "        current_retainer[0:dim_0,0:dim_1,i] = current_retainer[0:dim_0,0:dim_1,i-1]\n",
    "    current_retainer[0:dim_0,0:dim_1,0] = processed_new_image[0:dim_0,0:dim_1,0]\n",
    "    return(current_retainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_buffer(current_buffer, new_experience):\n",
    "    current_buffer.append(new_experience)\n",
    "    return(current_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experience_list(processed_current_state, current_action, current_reward, processed_next_state, stage_end_status):\n",
    "    experience_list = []\n",
    "    experience_list.append(processed_current_state)\n",
    "    experience_list.append(current_action)\n",
    "    experience_list.append(current_reward)\n",
    "    experience_list.append(processed_next_state)\n",
    "    experience_list.append(stage_end_status)\n",
    "    return(experience_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_maintainer(updated_buffer_enter):\n",
    "    if len(updated_buffer_enter) > buffer_capacity:\n",
    "        for mock_ite in np.arange(0,len(updated_buffer_enter) - buffer_capacity):\n",
    "            del updated_buffer_enter[0]\n",
    "    return(updated_buffer_enter)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_retainer(retainer_new_episode):\n",
    "    retainer_new_episode = torch.zeros(84,84,retention)       \n",
    "    return(retainer_new_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_nn(processed_retainer):\n",
    "    processed_retainer = np.transpose(processed_retainer,(2,0,1))\n",
    "    processed_retainer_fin = torch.zeros(1,retention,84,84)\n",
    "    processed_retainer_fin[0] = processed_retainer\n",
    "    return(processed_retainer_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = 5                  \n",
    "learning_rate = 5e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "CNN_q = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels=retention, out_channels=32, kernel_size=8, stride=4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Flatten(),\n",
    "                      nn.Linear(3136, 512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512, possible_actions),\n",
    "                      )\n",
    "optimizer_q = optim.Adam(CNN_q.parameters(), lr=learning_rate)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 25\n",
    "max_done_per_episode = 100\n",
    "epsilon = 0.9\n",
    "epsilon_decay = 0.99\n",
    "gamma = 0.97\n",
    "retainer = torch.zeros(84,84,retention)\n",
    "final_reward_array = []\n",
    "final_score_array = []\n",
    "final_x_array = []\n",
    "replay_buffer = []\n",
    "\n",
    "for i in np.arange(0,total_episodes):\n",
    "    \n",
    "    current_episode = i+1\n",
    "    initial_frame = env.reset()\n",
    "    env.render()\n",
    "    retainer = reset_retainer(retainer)\n",
    "    retainer = update_retainer(retainer,initial_frame)\n",
    "    reward_array = []\n",
    "    score_array = []\n",
    "    x_array = []\n",
    "    flag_status = False\n",
    "    done_status = False\n",
    "    times_done = 0\n",
    "    total_reward = 0\n",
    "    total_score = 0\n",
    "    total_x = 0 \n",
    "    if i >= 5:\n",
    "        epsilon = 0.7\n",
    "    else:\n",
    "        epsilon = 0.9\n",
    "    \n",
    "    while (flag_status == False) and (times_done < max_done_per_episode):\n",
    "        \n",
    "        if epsilon > 0.3:                                     \n",
    "            epsilon = epsilon*epsilon_decay                     \n",
    "        else:\n",
    "            epsilon = 0.3                                     \n",
    "            \n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_estimate = CNN_q(input_to_nn(retainer))\n",
    "            action = torch.argmax(q_estimate)\n",
    "            \n",
    "        aggregate_reward = 0\n",
    "        for j in np.arange(0,to_skip+1):\n",
    "            state, reward, done, info = env.step(int(action))\n",
    "            env.render()\n",
    "            aggregate_reward = aggregate_reward + reward\n",
    "            if (done == True) or (info['flag_get'] == True):\n",
    "                break\n",
    "        \n",
    "        \n",
    "        next_frame = state\n",
    "        done_status = done\n",
    "        flag_status = info['flag_get']\n",
    "        total_reward = total_reward + aggregate_reward\n",
    "        total_score = info['score']\n",
    "        total_x = info['x_pos']\n",
    "        \n",
    "        \n",
    "        next_retainer = update_retainer(retainer,next_frame)\n",
    "        current_experience = make_experience_list(retainer,action,aggregate_reward,next_retainer,flag_status)\n",
    "        replay_buffer = update_buffer(replay_buffer,current_experience)\n",
    "        replay_buffer = buffer_maintainer(replay_buffer)\n",
    "        \n",
    "        \n",
    "        if len(replay_buffer) <= batch_size:\n",
    "            mini_batch = replay_buffer\n",
    "        else:\n",
    "            sampling_index = np.random.choice(np.arange(len(replay_buffer)),batch_size,replace=False)\n",
    "            mini_batch = []\n",
    "            for k in np.arange(batch_size):\n",
    "                mini_batch.append(replay_buffer[sampling_index[k]])\n",
    "\n",
    "\n",
    "        current_val = []\n",
    "        target = []\n",
    "\n",
    "\n",
    "        for indexer in np.arange(len(mini_batch)):\n",
    "            optimize_experience = mini_batch[indexer]\n",
    "            current_state_j = optimize_experience[0]\n",
    "            current_action_j = optimize_experience[1]\n",
    "            current_reward_j = optimize_experience[2]\n",
    "            next_state_j = optimize_experience[3]\n",
    "            flag_status_j = optimize_experience[4]\n",
    "            with torch.no_grad():\n",
    "                loc_current_val = CNN_q(input_to_nn(current_state_j))[0][current_action_j]\n",
    "                current_val.append(loc_current_val)\n",
    "                if flag_status_j == True:       \n",
    "                    loc_target = current_reward_j \n",
    "                else: \n",
    "                    loc_target = current_reward_j + gamma*(torch.max(CNN_q(input_to_nn(next_state_j))))\n",
    "                target.append(loc_target)\n",
    "\n",
    "\n",
    "        current_val = np.array(current_val)     \n",
    "        target = np.array(target)               \n",
    "\n",
    "        current_val_copy = np.copy(current_val)\n",
    "        target_copy = np.copy(target)\n",
    "\n",
    "        current_val_tensor = torch.tensor(current_val_copy,requires_grad=True)\n",
    "        target_tensor = torch.tensor(target_copy,requires_grad=True)\n",
    "        \n",
    "        current_val_tensor = current_val_tensor.float()\n",
    "        target_tensor = target_tensor.float()\n",
    "\n",
    "        loss = F.mse_loss(current_val_tensor,target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer_q.step()\n",
    "\n",
    "        retainer = next_retainer\n",
    "        \n",
    "        if (done_status == True) and (flag_status == False):\n",
    "            reward_array.append(total_reward)\n",
    "            score_array.append(total_score)\n",
    "            x_array.append(total_x)\n",
    "            initial_frame = env.reset()\n",
    "            env.render()\n",
    "            retainer = update_retainer(retainer,initial_frame)\n",
    "            total_reward = 0\n",
    "            total_score = 0\n",
    "            total_x = 0\n",
    "            done_status = False\n",
    "            flag_status = False\n",
    "            times_done = times_done + 1\n",
    "            \n",
    "            \n",
    "        if flag_status == True:\n",
    "            reward_array.append(total_reward)\n",
    "            score_array.append(total_score)\n",
    "            x_array.append(total_x)\n",
    "            \n",
    "    \n",
    "    reward_mean = np.mean(np.array(reward_array))\n",
    "    score_mean = np.mean(np.array(score_array))\n",
    "    x_mean = np.mean(np.array(x_array))\n",
    "    \n",
    "    final_reward_array.append(reward_mean)\n",
    "    final_score_array.append(score_mean)\n",
    "    final_x_array.append(x_mean)\n",
    "    \n",
    "    print('The episode completed is :',current_episode)\n",
    "    print('The average reward in this episode is :',reward_mean)\n",
    "    print('The average score in this episode is :',score_mean)\n",
    "    print('The average x position Mario reaches, without dying, is :',x_mean)\n",
    "    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,26),np.array(final_reward_array))\n",
    "plt.xlabel('No. of Episodes')\n",
    "plt.ylabel('The average reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,26),np.array(final_score_array))\n",
    "plt.xlabel('No. of Episodes')\n",
    "plt.ylabel('The average score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,26),np.array(final_x_array))\n",
    "plt.xlabel('No. of Episodes')\n",
    "plt.ylabel('The average x position of Mario')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
